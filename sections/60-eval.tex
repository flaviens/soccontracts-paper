In this section, we evaluate \ourname in terms of cost, snippet scoring, coverage, and bug discovery.
We first evaluate the number of iterations required to generate valid SystemVerilog inputs and deduce the token usage (Section~\ref{sec:eval:iterations}).
We then evaluate the scoring function used to evaluate each base snippet (Section~\ref{sec:eval:scoring}),
the coverage achieved by \ourname and compare it with the state-of-the-art fuzzers (Section~\ref{sec:eval:coverage}).
We finally evaluate the ability and performance of \ourname in discovering new bugs (Section~\ref{sec:eval:newbugs}).

\para{Evaluation setting}
The performance results were obtained on a machine equipped with two Intel E5-2667v4 processors at 3.2~GHz equipped with 16 logical cores and 256~GB of DRAM.
As an LLM, we use OpenAI's o3 model, which provides the best responses for the purposes of this work as indicated by the experimental results gathered in Section~\ref{sec:llms}.
When performing differential fuzzing, we supply 1000 random stimuli and check whether there is any output mismatch in simulation.
We implemented \ourname as 10\,k lines of golang code, and provide 11\,k lines of unit tests.
In particular, the parser consists of 2.5k lines of code, while the printer, the combinator and the testbench generator are each approximately 1k lines of code long.

\para{Versions}
The versions of the tools used are those available on github as of August 2025, and were compiled from source as the tools were patched.
The exact versions are in Appendix~\ref{sec:appendix:tools-versions}.
For the fuzzer generated files we used the database of fuzzing files generated by the chimera project~\cite{chibench}.


% We use microbenchmarks to quantify the speed of program construction, and compare it with previous work (Section~\ref{sec:microbench}).
% We evaluate the program metrics for \ourname (Section~\ref{sec:evalmetrics}) similar to what we did for DifuzzRTL in Section~\ref{subsec:observations} .
% We compare the coverage achieved by \ourname according to multiple coverage metrics and compare it with the state-of-the-art fuzzers that specifically target these metrics (Section~\ref{sec:coverage_eval}).
% We describe the \numcpubugs new bugs found in 5 of the 6 evaluated \riscv CPUs and in the Yosys synthesizer (Section~\ref{subsec:bug_discoveries}) and provide a full list in Appendix~\ref{sec:appendix_bugtable}.
% Finally, we evaluate the performance of program reduction (Section~\ref{sec:reductperf}).

\subsection{Iterations and token usage}
\label{sec:eval:iterations}

On average it takes less than 5 iterations to generate a valid SystemVerilog file by openai o3 or o4-mini, and less than 10 iterations with Gemini 2.5 Flash.
All the experiments and evaluations described in this paper cost approximately 50 US dollars worth of api tokens.

\subsection{Scoring}
\label{sec:eval:scoring}

\subsubsection{Score distribution}
\label{sec:eval:score-distribution}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{plots/scores/scores_histogram.pdf}
    \vspace{-2em}
    \caption{Repartition of the snippets for total score varying between 0 and 1.}
    \label{fig:sscr}
    \vspace{-1em}
\end{figure}

% \para{Methodology}
We perform the methodology described in Section~\ref{sec:disparities:scoring}.
For all the snippets generated by the LLM generator, we ran each snippet against each tool.
For each simulator that managed to elaborate the code we added a point,
for each elaborated code that could be simulated correctly we added another point,
and for each synthesizer that could synthesize the code we added a third point.
We then divided by the maximal number of points ($2 \times $ number of simulators + number of synthesizers) to get a score between 0 and 1.
We used four simulators (Icarus Verilog, Verilator, CXXRTL with Yosys's frontend, and CXXRTL with Slang's frontend) and one synthesizer (Yosys),
and one parsing tool that we assimilated as a synthesizer sv2v~\cite{sv2v}.

\para{Results}
Figure~\ref{fig:sscr} shows the score distribution of the base snippet scores.
We can see that approximately 70\% of our snippets are supported by all the tools and the remaining 30\% are spread between the score of 0.2 and the score of 0.9.

\subsubsection{Evaluation of the model}
\label{sec:eval:disparities-evaluation}

% \para{Methodology}
To evaluate the effectiveness of our probabilistic model and its hypothesis, we ran a series of experiments in which we generated 500 files for a minimal score value of 0.75 each.

\para{Result}
We obtained an average score of 0.82536 (with a standard deviation of 0.1286), exceeding our target of 0.75.
The gap between the expected and observed results stems from our simplifying assumption that the score represents a probability of support by a tool.
In reality, each tool behaves deterministically for a given program, so treating support as independent probabilities underestimates actual coverage.
The overlap between tool functionalities is thus better captured by a Venn-diagram view than by random distribution.
Still, our main objective, which is avoiding simulation collapse, is achieved by this model.
Further details on the probabilistic model appear in Appendix~\ref{sec:appendix:proof}.

\subsection{Coverage}
\label{sec:eval:coverage}

\begin{table}[t]
    \centering
    \caption{Snippet generation by \ourname starting from an empty file. Top: number of accepted input files by the respective EDA tool. Bottom: cumulated coverage of the accepted input files (function/line/branch).}
    \label{tab:simple-vf-cov}
    \small
    \begin{tabular}{|c|c|c|}
        \hline
        \rowcolor{gray!10} % light gray background
        \textbf{Verilator} & \textbf{Yosys}   & \textbf{Slang}   \\
        \hline
        39/50              & 41/50            & 41/50            \\
        51.8/45.3/35.2\%   & 24.6/14.5/15.7\% & 21.0/21.2/15.6\% \\
        \hline
    \end{tabular}
\end{table}

Table~\ref{tab:simple-vf-cov} reports \ourname results for 50 input files generated from an empty seed (unlike the next experiment). Coverage was measured only on files accepted by Verilator.
Overall, coverage is higher than with traditional procedural fuzzers for the same number of files.
The snippets were produced by Gemini 2.5 Flash.
While Verilator coverage matches that of the snippets in Table~\ref{tab:feedback-llm-results}, Slang coverage is lower, likely because many Slang-validated snippets were actually invalid, scored poorly, and were thus excluded from these 50 files.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{plots/coverage_evolution_verifuse/coverage_evolution_verilator.pdf}
    \caption{Verilator coverage for \ourname (VF) for lines (L), branches (B) and functions (F). Verismith (VS) executed for 24 hours serves as a baseline.}
    \label{fig:verilatorcoverage}
\end{figure}


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{plots/coverage_evolution_verifuse/coverage_evolution_yosys.pdf}
%     \caption{Yosys coverage for \ourname (VF) for lines (L), branches (B) and functions (F). Verismith (VS) executed for 24 hours serves as a baseline.}
%     \label{fig:yosyscoverage}
% \end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{plots/coverage_evolution_verifuse/coverage_evolution_slang.pdf}
    \caption{Slang coverage for \ourname (VF) for lines (L), branches (B) and functions (F). Verismith (VS) executed for 24 hours serves as a baseline.}
    \label{fig:slangcoverage}
\end{figure}


Figure~\ref{fig:verilatorcoverage} shows coverage when generating one file per base snippet and measuring after each elaboration.
% We used this approach rather than starting from an empty file to speed up convergence.
The results demonstrate that our SystemVerilog generator steadily increases tool coverage.
Compared to Figure~\ref{fig:prev-coverage}, coverage increases by between \covboostlowverilator\% and \covboosthighverilator\%, and after only seven generated files we surpass Verismith, the best-performing fuzzer regarding coverage in prior work, according to Section~\ref{sec:motivation}.
Similar conclusions can be made for Slang when Figure~\ref{fig:slangcoverage}, with a coverage increase between \covboostlowslang\% and \covboosthighslang\%.
% Nevertheless concerning yosys, the coverage reaches the previous levels and slightly overtakes them.
% The poor results for yosys are simply explainable by the fact that no snippets were generated by the LLM targeting this tool.
% We only did so for Slang and verilator as of August 2025.



% \subsection{Time to bugs}

% \label{sec:eval:timetobugs}

% \textcolor{cyan}{TODO JN.}

\subsection{New bugs}
\label{sec:eval:newbugs}

We now present the bugs discovered by \ourname, first by leveraging base snippets in isolation (Section~\ref{sec:codegen}), then by combining them (Sections~\ref{sec:complex-code} and~\ref{sec:disparities}).
% We provide the corresponding mistranslation gadgets~\cite{mirtl} in Appendix~\ref{sec:appendix:mistranslation-gadgets}.
% \fls{TODO Add the mistranslation gadgets}

\subsubsection{Bugs from isolated base snippets}
\label{sec:eval:basesnippetbugs}

Table \ref{tab:llm-bugs} regroups the translation bugs discovered by leveraging the base snippets without combining them.
Most of these bugs are either directly features badly supported by the parsers,
or features that are sufficiently noteworthy to be explicitly addressed in the code of a given tool.
Therefore the tools who do not address this issue are likely to produce incorrect results when encountering these features.

\subsubsection{Bugs from combined snippets}
\label{sec:eval:basesnippetbugs}

Table \ref{tab:combined-bugs} summarizes the translation bugs discovered by leveraging base snippets without combining them. We identified three main reasons why these bugs were not directly found through base snippets. First, combining different types of code, such as combinational, sequential, and behavioral constructs, that are usually employed at different stages of the EDA flow, but not within the same file, revealed issues the LLM alone did not surface. Second, certain ambiguities in the IEEE standard lead to differing tool interpretations, which the LLM may implicitly avoid. Finally, some bugs arose from exploitable defaults in tool behavior. These defaults are implementation choices made by tool developers, not defined in the IEEE standard, and are often reinforced by the fact that most LLM-generated code conforms to these assumptions.

Table \ref{tab:combined-bugs} summarizes the bugs that were only found by combining snippets, and not found by the individual snippets.
These bugs can be used to create simulator oracles, and thus as MiRTL trojans.

\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}


\begin{table}[t]
    \centering
    \caption{Bugs discovered with base snippets in isolation.}
    \label{tab:llm-bugs}
    \small
    \begin{tabular}{|P{0.15\linewidth}|p{0.8\linewidth}|}
        \hline
        \rowcolor{gray!10} % light gray background
        \textbf{Bug}                     & \textbf{Description}                                                               \\
        \hline
        % Yosys \#5151      & \href{https://github.com/YosysHQ/yosys/issues/5151}{Yosys: Incorrect handling of post-decrement operation in \texttt{always\_comb}}          \\
        % \hline
        % Yosys \#5157      & \href{https://github.com/YosysHQ/yosys/issues/5157}{Yosys: \texttt{read\_verilog}: \texttt{inout} parameters not copied out of tasks}        \\
        % \hline
        % Slang \#161       & \href{https://github.com/povik/yosys-slang/issues/161}{yosys-slang: Incorrect handling of post-decrement operation in \texttt{always\_comb}} \\
        % \hline
        % Slang \#212       & \href{https://github.com/povik/yosys-slang/issues/212}{yosys-slang: Incorrect handling of \texttt{wor}}                                      \\
        % \hline
        % Xcelium 2           & Bad support of sensitivity list for class instantiations in \texttt{always\_comb} (\href{https://www.edaplayground.com/x/ZXFT}{example})     \\

        Yosys \#5151                     & Incorrect handling of post-decrement operation in \texttt{always\_comb}.           \\
        \hline
        Yosys \#5157                     & \texttt{read\_verilog}: \texttt{inout} parameters not copied out of tasks.         \\
        \hline
        Slang \#161                      & Incorrect handling of post-decrement operation in \texttt{always\_comb}.           \\
        \hline
        Slang \#212                      & Incorrect handling of the \texttt{wor} (wired-OR) net type.                        \\
        \hline
        Xcelium 2, VCS1, Aldec1, Questa1 & Bad support of sensitivity list for class instantiations in \texttt{always\_comb}. \\
        \hline
    \end{tabular}
\end{table}

\begin{table}[t]
    \centering
    \caption{Bugs discovered only through combined snippets.}
    \label{tab:combined-bugs}
    \small
    \begin{tabular}{|P{0.15\linewidth}|p{0.8\linewidth}|}
        \hline
        \rowcolor{gray!10} % light gray background
        \textbf{Bug}     & \textbf{Description}                                                                              \\
        \hline
        % V\#6199      & \href{https://github.com/verilator/verilator/issues/6199}{Verilator: Incorrect handling of \texttt{x} in if blocks (\texttt{if\ (x)\ else\ })} \\
        % X1           & Scheduling semantics violation (\href{https://www.edaplayground.com/x/7ySV}{example})                                                          \\
        % Y\#5238      & \href{https://github.com/YosysHQ/yosys/issues/5238}{Yosys: Incorrect handling of \texttt{x} in if blocks (\texttt{if\ (x)\ else\ })}           \\

        Verilator \#6199 & Incorrect handling of unknowns when used in conditional blocks.                                   \\
        \hline
        Xcelium 1        & Scheduling semantics violation.                                                                   \\
        \hline
        Yosys \#5238     & Incorrect handling of unknowns when used in conditional blocks.                                   \\
        \hline
        CXXRTL \#5161    & Indirect clock handling.                                                                           \\
        \hline
        Yosys \#5212     & Multiple drivers on a variable in a module change values of input ports when using \texttt{prep}. \\
        \hline
    \end{tabular}
\end{table}

\subsubsection{Other bugs} \label{sec:eval:lesserqualitybugs}

% <<<<<<< HEAD
The fuzzer additionally uncovered 7 lower-priority issues exclusively found from base snippets.
Specifically, two of these issues are design defects that can be used for MiRTL, another case was identified but is not exploitable for MiRTL.
% =======
% The fuzzer additionally uncovered nine lower-priority issues: two of them were found by combining snippets, and seven arose purely from base snippets.
% Specifically, three of these issues are design defects that can be used for MiRTL, one corresponds to a previously known issue, and another case was identified but is not exploitable for MiRTL.
% >>>>>>> e0d0460 (WIP)
In addition, two deviations from common initialization conventions were observed in two different simulators.
Another issue highlights an ambiguity in the definition of an edge, whether it denotes attainment of a high level or instead a low-to-high transition.
Finally, one case involved division by zero; however, no issue was filed in that case.
Because Verilog models hardware, division by zero does not trigger a runtime failure.
Simulators instead adopt tool-specific behaviors.
While such behavior can serve as a simulator oracle, it reflects undefined behavior rather than a true bug.
Moreover, formal verification tools should detect the possibility of division by zero and issue warnings about the implied default.

% <<<<<<< HEAD
% {\color{red}TODO JN
%     The fuzzer additionally uncovered nine lower-priority issues two by combining the snippets, and seven purely from base snippets.
%     Specifically:
%     \begin{itemize}
%         \item two design defects, that can be used for MiRTL;
%         \item one case not exploitable for MiRTL;
%         \item two deviations from common initialization conventions observed in two simulators;
%         \item one ambiguity in the definition of an edge---whether it denotes attainment of a high level or a low-to-high transition;
%         \item one case involving division by zero, for which no issue was filed.
%     \end{itemize}
%     Because Verilog models hardware, division by zero does not trigger a runtime failure; simulators therefore adopt tool-specific behaviors.
%     While such behavior can be used as a simulator oracle, it reflects undefined behavior rather than a bug. Moreover, formal verification tools should flag the possibility of division by zero and warn about the implied default.

%     Here are the issues of these other bugs:
%     \begin{itemize}
%         \item Convention on initialisation \#1254 Simulation error in always @* block ?
%         \item Transfuzz bugs not fixed yet \#4151 \texttt{opt\_muxtree} broken optimisation
%     \end{itemize}
% }
% =======
The other bugs uncovered correspond to specific issues already tracked.
These include a known design defect in CXXRTL (\#5161) related to indirect clock handling, Yosys issue \#5212 in which multiple drivers on a variable in a module cause changes to input ports when using the prep command, a convention on initialization problem (\#1254) reported as a simulation error in an \texttt{always @*} block, and a Transfuzz-related issue (\#4151) involving a broken optimization in \texttt{opt\_muxtree}.

% <<<<<<< HEAD
%     Here are the issues of these other bugs:
%     \begin{itemize}
%         \item known design defect - \#5161 CXXRTL - indirect clock handling
%         \item \#5212 Yosys - multiple
%               drivers on a variable in a module change values of input ports when
%               using \texttt{prep}
%         \item Convention on initialisation \#1254 Simulation error in always @* block ?
%         \item Transfuzz bugs not fixed yet \#4151 \texttt{opt\_muxtree} broken optimisation
%     \end{itemize}
% }
% >>>>>>> d04e70b (WIP)

\subsection{Time to bug}\label{sec:eval:timetobugs}

LLM-related bugs were detected immediately during the determinism check.
Issues with \texttt{X}/\texttt{Z} values appeared within 30 minutes, and the scheduling semantics bug occurred intermittently over 24 hours.
All remaining bugs were discovered in under an hour, with every issue reproduced multiple times during the 24-hour run.

% =======
% The fuzzer additionally uncovered nine lower-priority issues two by combining the snippets, and seven purely from base snippets.
% Specifically:
% \begin{itemize}
%     \item three design defects, that can be used for MiRTL;
%     \item one previously known issue;
%     \item one case not exploitable for MiRTL;
%     \item two deviations from common initialization conventions observed in two simulators;
%     \item one ambiguity in the definition of an edge---whether it denotes attainment of a high level or a low-to-high transition;
%     \item one case involving division by zero, for which no issue was filed.
% \end{itemize}
% Because Verilog models hardware, division by zero does not trigger a runtime failure; simulators therefore adopt tool-specific behaviors.
% While such behavior can be used as a simulator oracle, it reflects undefined behavior rather than a bug. Moreover, formal verification tools should flag the possibility of division by zero and warn about the implied default.

% Here are the issues of these other bugs:
% \begin{itemize}
%     \item known design defect - \#5161 CXXRTL - indirect clock handling
%     \item \#5212 Yosys - multiple
%             drivers on a variable in a module change values of input ports when
%             using \texttt{prep}
%     \item Convention on initialisation \#1254 Simulation error in always @* block ?
%     \item Transfuzz bugs not fixed yet \#4151 \texttt{opt\_muxtree} broken optimisation
% \end{itemize}
% >>>>>>> 7e704f0 (WIP)
