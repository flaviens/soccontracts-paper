In this section, we evaluate the ability of LLMs to generate code that achieves better coverage than traditional EDA software fuzzers, and we identify resulting challenges.

\para{LLMs and EDA software under study}
We consider Gemini 2.5 Flash Preview, o3 and o4-mini, which are state-of-the-art commercial LLMs.
In this initial study, we consider the same target EDA software as in Section~\ref{sec:motivation}, i.e.,
Verilator~\cite{Snyder2024Verilator}, Yosys~\cite{wolf2013yosys}, and Slang~\cite{popoloski2019slang}, which form a representative set of EDA software that takes rich SystemVerilog as inputs.

\subsection{Monolithic LLM Experiment}
\label{sec:llms:monolithic-llm}

\para{Methodology}
We prompt the LLMs to produce diverse SystemVerilog snippets for fuzzing EDA software.
Along with the prompt, we provide a randomly sampled source file from the target EDA software along with its associated coverage data from previous fuzzing iterations.
We provide the prompt in Listing~\ref{lst:system-prompt-template} in Appendix~\ref{sec:appendix:prompts}.
We generate 50 such SystemVerilog files, then submit them to the target EDA software.
This first experiment is conducted without feedback.

\begin{table}[t]
    \centering
    \caption{Monolithic LLM experiment results across EDA applications. Top: number of accepted input files by the respective EDA tool. Bottom: cumulated coverage of the accepted input files (function/line/branch).}
    \label{tab:monolithic-llm-results}
    \small
    \begin{tabular}{|l|c|c|c|}
        \hline
        \rowcolor{gray!10} % light gray background
        \textbf{} & \textbf{2.5 Flash} & \textbf{o3}      & \textbf{o4-mini} \\
        \hline
        \multirow{2}{*}{Verilator}
                  & 2/50               & 12/50            & 9/50             \\
                  & 51.1/45.2/31.6\%   & 55.1/50.0/35.8\% & 55.3/47.9/33.6\% \\
        \hline
        \multirow{2}{*}{Yosys}
                  & 0/50               & 0/50             & 0/50             \\
                  & N.A.            & N.A.          & N.A.          \\
        \hline
        \multirow{2}{*}{Slang}
                  & 5/50               & 15/50            & 8/50             \\
                  & 23.2/23.6/17.8\%   & 24.7/26.5/19.5\% & 23.3/23.8/17.9\% \\
        \hline
    \end{tabular}
\end{table}


\para{Results}
Table~\ref{tab:monolithic-llm-results} summarizes the number of files correctly generated by the LLMs and the cumulative coverage obtained by the SystemVerilog designs that were valid (Files rejected by the tools are discarded, as they cannot serve in differential fuzzing).

We observe that LLMs are generally incapable of generating SystemVerilog code accepted by the EDA software under study in a single prompt.
However, for the few cases that were accepted by the respective EDA software, we observe promising coverage numbers.
Indeed, in 24 hours, Verismith achieved 48.1\%, 40.2\%, and 29.4\% coverage for function, line, and branch coverage, respectively on Verilator.

\subsection{LLM Experiment with feedback}
\label{sec:llms:feedback-llm}

\begin{table}[t]
    \centering
    \caption{LLM experiment results with feedback across EDA applications. Top: number of accepted input files by the respective EDA tool. Bottom: cumulated coverage of the accepted input files (function/line/branch).}
    \label{tab:feedback-llm-results}
    \small
    \begin{tabular}{|l|c|c|c|}
        \hline
        \rowcolor{gray!10} % light gray background
        \textbf{} & \textbf{2.5 Flash} & \textbf{o3}      & \textbf{o4-mini} \\
        \hline
        \multirow{2}{*}{Verilator}
                  & 3/50               & 17/50            & 16/50            \\
                  & 51.1/45.5/31.6\%   & 59.8/54.6/39.0\% & 57.1/50.9/36.4\% \\
        \hline
        \multirow{2}{*}{Yosys}
                  & 0/50               & 2/50             & 2/50             \\
                  & N.A.            & 25.6/15.6/16.3\% & 22.7/12.3/13.2\% \\
        \hline
        \multirow{2}{*}{Slang}
                  & 41/50              & 49/50            & 42/50            \\
                  & 37.3/42.8/34.3\%   & 34.7/39.3/31.3\% & 32.0/34.9/27.3\% \\
        \hline
    \end{tabular}
\end{table}

\para{Methodology}
Recent advancements in LLMs for code generation have proposed to generate code in several rounds~\cite{jiang2024llmfuzz,ossfuzzgen}.
We adapt this idea by implementing a feedback loop where the LLM can iteratively improve its code generation based on validation results.
For this experiment, the feedback is provided by Slang, as it supports the widest range of syntactic constructs.
We send the errors back to the model and let it try to correct the file for 5 iterations, then validate each file against the tools, measuring coverage for the valid ones.
Like in Section~\ref{sec:llms:monolithic-llm}, we attempt to generate 50 SystemVerilog files.
% , trying to maximise the coverage of Verilator.
The prompts used in this process are provided in Listings~\ref{lst:initial-prompt-template} and~\ref{lst:feedback-prompt-template} in Appendix~\ref{sec:appendix:prompts}.

\para{Results}
Table~\ref{tab:feedback-llm-results} summarizes the number of files that were correctly generated by the LLMs,
as well as the cumulated coverage obtained by the SystemVerilog designs that were valid.
% \textcolor{cyan}{Interestingly we did not attempt to validate the files by Verilator which provided very poor results because much of the code generated is not simulatable despite being syntactically correct.}
Nevertheless, the coverage results exceed the classical procedural fuzzers' in each test, except when no LLM-generated files were valid.
% Which shows us very clearly that the LLMs are capable of generating high-coverage SystemVerilog code, despite it not being always valid.
% Running the feedback loop for 10 iterations yields slight improvements in the number of files generated but masked the difference of ease of correcting between models.
% Lastly we can see that if gemini seems to have trouble correcting the files, it nevertheless seems to yield better code diversity.
% \textcolor{cyan}{TODO JN Say that the coverage results are already very good.}
% \fls{What about coverage-based feedback for LLMs where we ask for covering a specific location of the code?}

\subsection{Hypothesis}
\label{sec:llms:lessons}

% \para{Validity and Coverage}
We have shown that LLMs can generate SystemVerilog code snippets that significantly improve coverage of the target EDA software.
However, previous fuzzing effort has insisted on the requirement for the diversity of constructs that are generated in the fuzzing process.
In Appendix~\ref{sec:appendix:base-snippet-examples}, we show examples of the snippets produced in Section~\ref{sec:llms:feedback-llm}.
Clearly, in each example, the LLMs produce a homogeneous set of constructs that lack the necessary intra-test-case diversity, which might impair their effectiveness in finding intricate bugs.
Indeed, LLMs have been trained on human-written or generated SystemVerilog code, which is typically homogeneous in terms of structure and style, within a given file.
% 
This leads to the hypothesis that guides \ourname's design.

\begin{newhypothesis}
    Combining LLMs SystemVerilog generation capabilities with traditional fuzzing techniques might foster the intra-file diversity of SystemVerilog code snippets and uncover bugs that are not found by neither the LLMs nor the fuzzing techniques alone.
\end{newhypothesis}
