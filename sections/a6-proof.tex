Our objective is to generate snippets that balance tool acceptance with syntactic diversity, while exploring the trade-offs between snippet diversity and acceptance.

We set the number of base snippets to combine to follow a geometric law of parameter \(g\).
Let us consider that we take with a probability \(p\) a consensual snippet and perform
two experiments, one to know if we are picking a consensual module, one to
know which one we are picking amongst the designated group.
The second
group will be considered to be uniformly scattered. In figure~\ref{fig:sscr} it is represented between values \(0.9\) and \(0.2\) and has
thus a mean close to \(\mu=0.5\) in our case. In practice this value was computed at startup, when loading the snippets.
Thus the probability for
having a valid file is \(p \times 1 + (1-p) \times \mu\).
If we repeat
this experiment \(i\) times identically, the probability of having a
valid file is \((p + \mu (1-p))^i\).

Now as both experiments are independent, and the latter are a partition of the universe,
then the global probability of having a valid file is the sum of the product of the two probabilities.
Furthermore as we are repeating this experience
according to the geometric law of parameter \(g\) with the subtelty that
we always inject at least one module.

The probability of having a valid file, is now by the law of total probabilities, and by the independence of the two experiments:

\[
    P = \sum_{i=0}^{+\infty} (p + \mu (1-p))^{i+1} (1-g)^{i} g
\]


\begin{align*}
    P & = \sum_{i = 0}^{+\infty} ((1-p) \times \mu + p \times 1)^{i+1}\times (1-g)^ig \\
    P & = ((1-p) \mu + p )g \sum_{i = 0}^{+\infty} ((1-g)(\mu (1-p) + p ))^{i}        \\
    P & = g \frac {(1-p) \mu + p }{1 - (1-g)(\mu (1-p) + p)}                          \\
\end{align*}
Now as this probability is that of a Bernoulli experiment, the expected value of having a valid file is
the value of the probability itself.
So finally if I want \(P \geq \nu\), we now have a much much simpler
relationship of
\[ p \geq \frac {\nu(1-\mu) + g \mu (\nu - 1)}{(1 - \mu)(g + \nu - \nu g)}\]

Finally now we have an inequality governing \(p\) according to \(g\),
\(\nu\) and \(\mu\), so we fix \(\nu\) to \(0.75\), \(\mu\) is measured
by our program at startup when loading the snippets and their scores.
But we need to decide of a value for \(g\).
We could pick g randomly
between \(0\) and \(1\) but we wanted to have a higher probability of
having a large number of snippets injected so decided to go for
\(0.9 \times (1-x)^3 + 0.1\) which will rapidly decrease starting at the
value of 1 and being nevertheless capped at \(0.1\).
These are choices
that can be debated but ensured that the expected value of injected
modules \(1 + 1/g\) would not skyrocket and that the probability \(p\)
to be not too high.
Finally we picked \(p\) as the lowest value that
validated the inequality above.
