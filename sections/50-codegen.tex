In this section, we address the coverage-guided generation of base snippets that can be used individually and will later be combined to create more complex hardware representations.


\subsection{Short snippets}
\label{sec:codegen:short-snippet-generation}

We have shown in Section~\ref{sec:llms} that LLMs are a promising approach to generate hardware representations for fuzzing EDA software,
but they are known to have limited ability to generate non-trivial hardware descriptions~\cite{hdl2v,10.1145/3643681} and particularly struggle with intra-file heterogeneity as discussed in Section~\ref{sec:llms:lessons}.
We therefore prompt an LLM to generate short behavioral snippets of SystemVerilog code.
% These base snippets can be a building block for the generation of valid and complex hardware representations.

\begin{newdesignprinciple}
    Leverage LLMs to generate short behavioral snippets of HDL code.
\end{newdesignprinciple}


\subsection{Coverage-based short snippet generation}
\label{sec:codegen:Coverage-snippet-generation}

\para{Coverage measurement}
To measure source code coverage of the target EDA tools, we compiled Yosys, Slang, and Verilator with coverage flags, then measured the coverage with \texttt{fastcov}~\cite{Gillespie_fastcov_2024}, which is a \texttt{gcov}~\cite{GCC_gcov_14_1_0} wrapper.
%
We also excluded from the coverage measurement files that are associated with functionalities that are not prone to causing translation bugs, such as test and debug files.
Appendix~\ref{sec:appendix:coverage-commands} provides the commands we used for collecting coverage.

\para{Coverage monitoring}
We prompt the LLM to generate short code snippets that target randomly sampled coverage points not yet exercised at a given fuzzing iteration.
Appendix~\ref{sec:appendix:prompts} provides the prompts we used.
% and the detailed code used to generate the snippets is also available in \url{https://github.com/toby-bro/instrumentedVerilator}
% \fls{\textcolor{cyan}{TODO JN: Provide the prompt in appendix.}}

\para{Coverage initialization}
We aim at finding unknown bugs.
Hence, we choose to initially execute Verismith and TransFuzz on the target EDA software for 100 iterations each to measure initial coverage unlikely to expose new bugs.
% We are not interested in covering locations that previous fuzzers such as Verismith and TransFuzz have already covered,
% although we will show in Section~\ref{sec:eval} that \ourname, despite explicitly trying to diverge from these tools,
% is also capable of finding the bugs that they discovered.
% We focus on the lines, branches and functions that have not been covered by Verismith and TransFuzz.

\para{Cross-software coverage}
We create a global pool of base snippets.
Hence, a snippet that was used to improve the coverage of one EDA software will also be tested on the other target EDA software.
This has several intuitive benefits.
It enables fuzzing closed-source software and helps identify unimplemented functionality, such as corner cases.
If one EDA tool omits a corner case, another is likely to implement it, making it a useful coverage target.
Finally, some EDA tools are oriented towards the comprehensiveness of SystemVerilog language support, such as Slang~\cite{popoloski2019slang}, while others are oriented towards performance, such as Verilator~\cite{Snyder2024Verilator}.
Considering coverage across diverse EDA tools, and later combining them, might add intricacies in the generated test cases, offering more comprehensive testing perspectives.

\begin{newdesignprinciple}
    Leverage cross-EDA software coverage measurement to guide the base snippet generation.
\end{newdesignprinciple}

\subsection{Snippet validation}
\label{sec:codegen:short-snippet-validation}

\para{Validation}
We run the generated code with the EDA software under study to ensure its validity, i.e., that the target EDA software for which coverage has been measured indeed accepts it.
The snippets could be invalidated for many reasons such as syntax errors,
or linting errors that tools like Slang or Verilator detect.
% Appendix~\ref{sec:appendix:snippet-examples}'s Listing~\ref{lst:refused} presents an example of a SystemVerilog-compliant code snippet that is refused by Verilator because of stricter linting rules.

\para{Coverage improvement validation}
Over 24-hour experiments, all snippets improved coverage.
Therefore, we do not measure coverage improvement at every iteration.

\para{Correction}
If the code is rejected by the EDA software under study, then we send it back to the LLM,
with the error log and again with the coverage excerpt of the code it needs to trigger.
Listing~\ref{lst:feedback-prompt-template} in Appendix~\ref{sec:appendix:prompts} shows the prompt we used.
We discard the individual base snippet generation process if the LLM cannot generate a valid code snippet after 10 iterations.

\begin{newdesignprinciple}
    Iteratively correct the generated code to make it accepted by the EDA software whose coverage is being used as feedback.
\end{newdesignprinciple}

Note that we do not require acceptance by all target EDA software, but only by the one whose coverage is being used as feedback for generating the current base snippet.
We will discuss disparities in SystemVerilog support in Section~\ref{sec:disparities}.


\subsection{Take-away}

This process generates short base snippets of code that improve the coverage of the EDA software under study.
These base snippets are intrinsically homogeneous and target a single coverage point in the target EDA software.
Appendix~\ref{sec:appendix:base-snippet-examples} provides example snippets generated for different coverage points in different tools.
